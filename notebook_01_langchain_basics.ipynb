{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM applications: Notebook 01\n",
    "\n",
    "# Langchain basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3.2:3b-instruct-fp16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama server: http://kqrw311-g5-12xlarge-a.img.astrazeneca.net:8080\n"
     ]
    }
   ],
   "source": [
    "# Read fro `.env` file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL')\n",
    "print(f\"Using Ollama server: {OLLAMA_URL if OLLAMA_URL else 'local'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 01: Connect to an LLM\n",
    "\n",
    "Connect to an LLM and make a simple request (e..g translate a senetence)\n",
    "\n",
    "```\n",
    "Translate to French: I like coffee.\n",
    "```\n",
    "\n",
    "Take a look at the LLM parameters, and full response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is the translation:\n",
      "\n",
      "J'aime le café.\n",
      "\n",
      "Or, in a more informal tone:\n",
      "\n",
      "Je bois beaucoup de café.\n",
      "\n",
      "Or, in a more casual tone:\n",
      "\n",
      "C'est mon café préféré!\n",
      "\n",
      "(Note: The first option is formal, the second is informal and polite, and the third is very casual and friendly.)\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "messages = \"Translate to French: I like coffee.\"\n",
    "\n",
    "# Invoke the LLM\n",
    "ret = llm.invoke(messages)\n",
    "ret.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Here is the translation:\\n\\nJ'aime le café.\\n\\nOr, in a more informal tone:\\n\\nJe bois beaucoup de café.\\n\\nOr, in a more casual tone:\\n\\nC'est mon café préféré!\\n\\n(Note: The first option is formal, the second is informal and polite, and the third is very casual and friendly.)\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"model\": \"llama3.2:3b-instruct-fp16\",\n",
      "    \"created_at\": \"2025-01-16T12:33:15.768067591Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 7661757204,\n",
      "    \"load_duration\": 20412346,\n",
      "    \"prompt_eval_count\": 33,\n",
      "    \"prompt_eval_duration\": 166000000,\n",
      "    \"eval_count\": 66,\n",
      "    \"eval_duration\": 7473000000,\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"\",\n",
      "      \"images\": null,\n",
      "      \"tool_calls\": null\n",
      "    }\n",
      "  },\n",
      "  \"type\": \"ai\",\n",
      "  \"name\": null,\n",
      "  \"id\": \"run-ab27dc7e-a409-4791-a9ef-321d46cedb02-0\",\n",
      "  \"example\": false,\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 33,\n",
      "    \"output_tokens\": 66,\n",
      "    \"total_tokens\": 99\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show details of the response\n",
    "print(ret.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 02: Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Je aime le café.\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "# Create a prompt from a template\n",
    "prompt_template = \"\"\"\n",
    "Translate to French:\n",
    "\n",
    "{sentence}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the string template\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Create a \"chain\" of runnable objects\n",
    "runnable = prompt | llm\n",
    "\n",
    "# Invoke the 'runnable'\n",
    "ret = runnable.invoke({\"sentence\": \"I like coffee.\"})\n",
    "ret.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 03: Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je aime le café.\n",
      "----\n",
      "Received 6 blocks\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "# Create a prompt from a template\n",
    "prompt_template = \"\"\"\n",
    "Translate to French:\n",
    "\n",
    "{sentence}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the string template\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Create a \"chain\" of runnable objects\n",
    "runnable = prompt | llm\n",
    "\n",
    "# Invoke the 'runnable' as a stream, print each block as it arrives\n",
    "blocks = []\n",
    "for b in runnable.stream({\"sentence\": \"I like coffee.\"}):\n",
    "    print(b.content, end='')\n",
    "    blocks.append(b)\n",
    "\n",
    "print(f\"\\n----\\nReceived {len(blocks)} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Je' additional_kwargs={} response_metadata={} id='run-7b7f84d9-db06-4e13-bb13-5395ea75b43b'\n"
     ]
    }
   ],
   "source": [
    "# What does a block look like\n",
    "print(blocks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 04: Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "J'aime le café. \n",
      "\n",
      "Note: I used a more formal way of saying \"I like coffee\" in French, which is suitable for writing or speaking with someone you don't know well.\n",
      "\n",
      "If you want to say it in an informal way (with friends or family), you can use:\n",
      "\n",
      "Je aime un café, ça fait plaisir!\n",
      "\n",
      "This means \"I love a cup of coffee, it's nice!\"\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "messages = [\n",
    "    ('system', \"You are helping to translate the user's inputs from english to french\"),\n",
    "    ('human', \"I like coffee\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "ret.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 04: Message prompt templates with roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Je aime le café.\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM object\n",
    "messages_template = [\n",
    "    ('system', \"You are helping to translate the user's inputs from English to {language}\"),\n",
    "    ('human', \"{sentence}\")\n",
    "]\n",
    "\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages_template)\n",
    "runnable = prompt | llm\n",
    "\n",
    "# Run the 'runnable'\n",
    "ret = runnable.invoke({\"language\": \"French\", \"sentence\": \"I like coffee.\"})\n",
    "ret.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
