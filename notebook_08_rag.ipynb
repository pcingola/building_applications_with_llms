{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM applications: Notebook 08\n",
    "\n",
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3.2:3b-instruct-fp16'\n",
    "# MODEL = 'llama3.3:70b-instruct-fp16'\n",
    "EMBEDING_MODEL = 'nomic-embed-text'\n",
    "\n",
    "DATA_DIR = Path('.') / 'data'\n",
    "CHROMA_PATH = DATA_DIR / 'chroma'\n",
    "DOCS_DIR = DATA_DIR / 'books'\n",
    "\n",
    "TOP_K = 5\n",
    "SCORE_THRESHOLD = 0.7\n",
    "\n",
    "_db_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama server: http://kqrw311-g5-12xlarge-a.img.astrazeneca.net:8080\n"
     ]
    }
   ],
   "source": [
    "# Read fro `.env` file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL')\n",
    "print(f\"Using Ollama server: {OLLAMA_URL if OLLAMA_URL else 'local'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=EMBEDING_MODEL, base_url=OLLAMA_URL)\n",
    "vdb = Chroma(persist_directory=str(CHROMA_PATH), embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Query the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7474655246167894\n",
      "Document: hhgttg-328\n",
      "Text: Zaphod Beeblebrox,  adventurer,  ex-hippy,  good  timer,  (crook?\n",
      "quite  possibly),  manic self-publicist, terribly bad at personal\n",
      "relationships, often thought to be completely out to lunch.\n",
      "\n",
      "Score: 0.5882808817417512\n",
      "Document: hhgttg-991\n",
      "Text: \"What do you mean you've  met?\"  he  demanded.  \"This  is  Zaphod\n",
      "Beeblebrox from Betelgeuse Five you know, not bloody Martin Smith\n",
      "from Croydon.\"\n",
      "\n",
      "Score: 0.5866329399649752\n",
      "Document: hhgttg-878\n",
      "Text: \"... and news brought to you here  on  the  sub-etha  wave  band,\n",
      "broadcasting  around  the  galaxy  around  the clock,\" squawked a\n",
      "voice, \"and we'll be saying a big hello to all  intelligent  life\n",
      "forms  everywhere  ... and to everyone else out there, the secret\n",
      "is to bang the rocks together, guys. And of course, the big  news\n",
      "story  tonight  is the sensational theft of the new Improbability\n",
      "Drive prototype ship by none other than Galactic President Zaphod\n",
      "Beeblebrox. And the question everyone's asking is ... has the big\n",
      "Z finally flipped? Beeblebrox,  the  man  who  invented  the  Pan\n",
      "Galactic  Gargle Blaster, ex-confidence trickster, once described\n",
      "by Eccentrica Gallumbits as the Best Bang since the Big One,  and\n",
      "recently  voted  the  Wort  Dressed  Sentinent Being in the Known\n",
      "Universe for the seventh time ... has he got an answer this time?\n",
      "We  asked his private brain care specialist Gag Halfrunt ...\" The\n",
      "music swirled and dived for a moment.  Another  voice  broke  in,\n",
      "presumably  Halfrunt.  He  said: \"Vell, Zaphod's jist zis guy you\n",
      "know?\" but got no further because an electric pencil flew  across\n",
      "the  cabin  and  through  the  radio's on/off sensitive airspace.\n",
      "Zaphod turned and glared at Trillian - she had thrown the pencil.\n",
      "\n",
      "Score: 0.5860539265601326\n",
      "Document: hhgttg-1936\n",
      "Text: \"We don't want to shoot you, Beeblebrox!\" shouted the figure.\n",
      "\"Suits me fine!\" shouted Zaphod back and dived down  a  wide  gap\n",
      "between two data process units.\n",
      "\n",
      "Score: 0.5524060743451573\n",
      "Document: hhgttg-334\n",
      "Text: Today was the day; today was the day when they would realize what\n",
      "Zaphod  had  been  up  to.  Today  was  what  Zaphod Beeblebrox's\n",
      "Presidency was all  about.  Today  was  also  his  two  hundredth\n",
      "birthday, but that was just another meaningless coincidence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Zaphod Beeblebrox?\"\n",
    "\n",
    "\n",
    "results = vdb.similarity_search_with_relevance_scores(question, k=TOP_K)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Document: {doc.id}\")\n",
    "    print(f\"Text: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add the results to the LLM prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following information, answer the quesiton below:\n",
    "\n",
    "{documents}\n",
    "\n",
    "Answer the quesiton below based on the previous information:\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the string template\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Create a \"chain\" of runnable objects\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "runnable = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided text, Zaphod Beeblebrox is the Galactic President of Betelgeuse Five who has stolen the new Improbability Drive prototype ship. He is also known for inventing the Pan Galactic Gargle Blaster, and has been described as an ex-confidence trickster with a long list of awards, including being voted the Worst Dressed Sentient Being in the Known Universe seven times.\n"
     ]
    }
   ],
   "source": [
    "# Join the results into a single string\n",
    "docs = \"\\n---\\n\".join([doc.page_content for doc, score in results if score < SCORE_THRESHOLD])\n",
    "\n",
    "# Invoke the llm model with the question and the documents\n",
    "ret = runnable.invoke({'question': question, 'documents': docs})\n",
    "\n",
    "ret.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
