{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM applications: Notebook 03\n",
    "\n",
    "# ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain.agents.output_parsers.react_single_input import ReActSingleInputOutputParser\n",
    "\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fro `.env` file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL')\n",
    "print(f\"Using Ollama server: {OLLAMA_URL if OLLAMA_URL else 'local'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create tools (`str` parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def double_of(n):\n",
    "    \"\"\"Double of a number\"\"\"\n",
    "    print(f\"Double of (type: {type(n)}) {n}\")\n",
    "    n = int(n)\n",
    "    print(f\"Double of {n}\")\n",
    "    return 2 * n\n",
    "\n",
    "\n",
    "@tool\n",
    "def factorial_of(n):\n",
    "    \"\"\"Factorial of a number\"\"\"\n",
    "    print(f\"Factorial of (type: {type(n)}) {n}\")\n",
    "    n = int(n)\n",
    "    fact = 1\n",
    "    for i in range(1, n+1):\n",
    "        fact *= i\n",
    "    return fact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugRunnable(Runnable):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    \"\"\" A runnable that just prints the input and returns it \"\"\"\n",
    "    def invoke(self, input, config=None, **kwargs):\n",
    "        print(f\"DebugRunnable {self.name}: (type: {type(input)})\")\n",
    "        if isinstance(input, list):\n",
    "            for i, item in enumerate(input):\n",
    "                print(f\"  {i}: {item}\")\n",
    "        elif isinstance(input, dict):\n",
    "            print(json.dumps(input, indent=2, default=str))\n",
    "        else:\n",
    "            print(str(input))\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 01: Calling tools manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, sub, double_of, factorial_of]\n",
    "tools_map = {t.name.lower(): t for t in tools}\n",
    "tools_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "# Bind the tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Create a message\n",
    "query = \"What is the addition of 40 and 2?\"\n",
    "message = HumanMessage(query)\n",
    "messages = [message]                # The list of messages to send to the LLM, now is only one message\n",
    "\n",
    "ret = llm_with_tools.invoke(messages)\n",
    "print(f\"Type: {type(ret)}\\n{ret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the tool calls\n",
    "ret.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the tools, append the results to the messages list\n",
    "for tool_call in ret.tool_calls:\n",
    "    tool_name = tool_call['name'].lower()\n",
    "    tool = tools_map.get(tool_name)\n",
    "    if tool:\n",
    "        tool_msg = tool.invoke(tool_call)\n",
    "        print(tool_msg)\n",
    "    print(f\"Tool message: {tool_msg}\")\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "# Show the messages\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLM with the messages (including the tool results)\n",
    "ret = llm_with_tools.invoke(messages)\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 02: AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [double_of, factorial_of]\n",
    "tools_map = {t.name.lower(): t for t in tools}\n",
    "tools_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "\n",
    "# Create the LLM object\n",
    "llm = ChatOllama(model=MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "# Create a prompt object from the string template\n",
    "prompt_template = \"\"\"Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentAction\n",
    "\n",
    "def format_log_to_str(\n",
    "    intermediate_steps: list[tuple[AgentAction, str]],\n",
    "    observation_prefix: str = \"Observation: \",\n",
    "    llm_prefix: str = \"Thought: \",\n",
    ") -> str:\n",
    "    \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\n",
    "    thoughts = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        thoughts += action.log\n",
    "        thoughts += f\"\\n{observation_prefix}{observation}\\n{llm_prefix}\"\n",
    "    return thoughts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# agent_debug = DebugRunnable('Before agent') | agent | DebugRunnable('After agent')\n",
    "\n",
    "llm_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    )\n",
    "    | prompt\n",
    "    | llm_with_stop\n",
    "    | ReActSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "# agent_executor = AgentExecutor(agent=agent_debug\n",
    "#                                , tools=tools\n",
    "#                                , return_intermediate_steps=True\n",
    "#                                , handle_parsing_errors=False\n",
    "#                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the factorial of 10?\"\n",
    "tool_names = ','.join([t.name for t in tools])\n",
    "\n",
    "agent_executor.invoke({'input': query, 'tool_names': tool_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
